---
title: "Chapter 7 Notes"
prefer-html: true
format: 
  docx: 
    toc: true
    highlight-style: arrow
    fig-format: retina
    fig-dpi: 300
    fig-height: 6
    fig-width: 6
editor: visual
execute:
  echo: fenced
  error: true
  output: true
editor_options: 
  chunk_output_type: inline
---

# Chapter 7 Notes: Spatial Analysis with US Census data

```{r setup}
library(tigris)
library(tidyverse)
library(sf)
library(tidycensus)
options(tigris_use_cache = TRUE)
```

-   defines *spatial analysis*Â as "performance of analytic tasks that explicitly incorporate the spatial properties of a dataset."

-   Positions R as a GIS/GIS Substitute

-   **sp** and **rgeos** as old standards; **sf** as new standard

-   Chapter focuses on integrating **sf**, **tidycensus**, and **tigris**

## 7.1 Spatial Overlay

-   defines *spatial overlay* as the layering of different geographic datasets with different geometries over each other to observe patterns and relations

### aligning coordinate reference systems

-   all layers need same CRS for spatial overlay

-   **tigris** and **tidycensus** datasets default to NAD 1983 geographic reference system

    -   **sf** uses **s2** for three-dimensional overlay

        -   This can slow things down compared to projected coordinate reference system

-   Author recommends following workflow:

    1.  Download datasets planned for use

    2.  identify appropriate CRS for layers (use `crsuggest::suggest_crs()`)

    3.  transform the dataset to projected coordinate system (use `sf::st_transform()`)

    4.  Compute spatial overlay

### Identifying geometries within a metropolitan area

-   Core-based statistical areas (i.e. metropolitan/micropolitan areas) often used for regional analysis

-   "Core-based statistical areas are defined as agglomerations of counties that are oriented around a central core or cores, and have a significant degree of population interaction as measured through commuting patterns."

    -   Metropolitan areas are CBSAs with pop \> 50K

#### Example: Kansas City metropolitan area

1.  use **tigris** to get 2020 census tracts for two states in Kansas City metro area

2.  get boundary of Kansas City metro area

```{r KC-metro}
library(tigris)
library(tidyverse)
library(sf)
options(tigris_use_cache = TRUE)

# Author uses NAD83(2011) Kansas Regional Coordinate System Zone 11)

ks_mo_tracts <- map_dfr(c("KS", "MO"), ~{
  tracts(.x, cb = TRUE, year = 2020)
}) %>% 
  st_transform(8528)

kc_metro <- core_based_statistical_areas(cb = TRUE, year = 2020) %>% 
  filter(str_detect(NAME, "Kansas City")) %>% 
  st_transform(8528)

ggplot() +
  geom_sf(data = ks_mo_tracts, fill = "white", color = "grey") +
  geom_sf(data = kc_metro, fill = NA, color = "red") +
  theme_void()
```

-   the red boundary shows the outline of the Kansas City Metropolitan area; the Grey boundaries represent census tracts.

-   We can extract the data within the red boundary using spatial subsetting

### Spatial Subsets and spatial predicates

-   subsetting uses the extent of one dataset to extract features from another dataset using *co-location* defined by a *spatial predicate*

    -   Spatial subsets can be expressed with base R indexing

```{r KSC-subsetting}
kc_tracts <- ks_mo_tracts[kc_metro, ]

ggplot() +
  geom_sf(data = kc_tracts, fill = "white", color = "grey") +
  geom_sf(data = kc_metro, fill = NA, color = "red") +
  theme_void()
```

-   This returns all census tracts *intersecting* the Kansas City Metro Area boundary

    -   the default spatial predicate is `st_intersects()`

-   If we want just those tracts *within* the boundary, we can use `st_within()` with `st_filter()` (or using base index syntax as above, but including `op = st_within` argument)

```{r KCM-within}
kc_tracts_within <- ks_mo_tracts %>% 
  st_filter(kc_metro, .predicate = st_within)

# Alternative:
kc_metro2 <- kc_tracts[kc_metro, op = st_within]

ggplot() +
  geom_sf(data = kc_tracts_within, fill = "white", color = "grey") +
  geom_sf(data = kc_metro, fill = NA, color = "red") +
  theme_void()
```

## 7.2 Spatial Joins

-   Spatial Joins transfer attributes between spatial layers

    -   where a table join uses a key field to join, spatial joins use spatial predicates

        -   **sf** uses `st_join()` for spatial joins

### Point-in-polygon spatial joins

-   Suppose we are a health data analysis

-   We need to determine \$ of residents 65+ w/o insurance in patients' neighborhoods

    -   we have patients IDs, lon, and lat for each 'patient'

```{r gainsville}
library(tidyverse)
library(sf)
library(tidycensus)
library(mapview)

gainesville_patients <- tibble(
  patient_id = 1:10,
  longitude = c(-82.308131, -82.311972, -82.361748, -82.374377, 
                -82.38177, -82.259461, -82.367436, -82.404031, 
                -82.43289, -82.461844),
  latitude = c(29.645933, 29.655195, 29.621759, 29.653576, 
               29.677201, 29.674923, 29.71099, 29.711587, 
               29.648227, 29.624037))
```

-   we need to convert the data to a simple features object

    -   `st_as_sf()` from **sf** takes a data frame or tibble with lon/lat and creates a point dataset

```{r FL-points}
gainesville_sf <- gainesville_patients %>% 
  st_as_sf(coords = c("longitude", "latitude"),
           crs = 4326) %>% 
  st_transform(6440) # transform to NAD83(2011) / Florida North

mapview(
  gainesville_sf,
  col.regions = "red",
  legend = FALSE)
```

-   Next, we acquire the health insurance data

    -   we have spatial & demo data, then we pre-process

        -   we call `select()` to retain three columns - GEOID, ACS estimate, and MOE

        -   we call `st_transform()` to allign the coordinate systems

```{r alachua-data}
alachua_insurance <- get_acs(
  geography = "tract",
  variables = "DP03_0096P",
  state = "FL",
  county = "Alachua",
  year = 2019,
  geometry = TRUE
) %>% 
  select(GEOID, pct_insured = estimate,
         pct_insured_moe = moe) %>% 
  st_transform(6440)
```

-   we have spatial & demo data; now we can pre-process prior to the spatial join

    -   we call `select()` to retain three columns - GEOID, ACS estimate, and MOE

    -   we call `st_transform()` to allign the coordinate systems

```{r interactive-layering}
mapview(alachua_insurance,
        zcol = "pct_insured",
        layer.name = "% with health<br/>insurance"
        ) +
  mapview(
    gainesville_sf,
    col.regions = "red",
    legend = FALSE
  )
```

-   to perform the join, we now move on to `st_join()`

    -   `st_join()` defaults to the `st_intersects()` predicate

```{r spatial-join}
patients_joined <- st_join(
  gainesville_sf,
  alachua_insurance
)
```

-   The resultant dataframe gives us the patients along with the rate (and moe) of uninsured in their respective census tracts

    -   be wary of the *ecological fallacy* - do not infer the individual is represented by group characteristics.

### Spatial joins and group-wise spatial analysis

-   we may also be interested in looking at areas within other areas

    -   i.e. neighborhoods within metropolitan areas

-   This can be achieved using *polygon-on-polygon spatial* joins

#### Spatial join data setup

-   We want to look at distribution of census tracts by Hispanic population for four largest metro areas in Texas

    -   We will use `B01003_001` from the 1-year ACS to get CBSA population data, and the geometry

```{r}
library(tidycensus)
library(tidyverse)
library(sf)

# CRS: NAD83(2011) / Texas Centric Albers Equal Area
tx_cbsa <- get_acs( # get the data
  geography = "cbsa",
  variables = "B01003_001",
  year = 2019,
  survey = "acs1",
  geometry = TRUE
) %>% filter(str_detect(NAME, "TX")) %>% # get those w/ names containing texas
  slice_max(estimate, n = 4) %>% # get the highest 4 estimates
  st_transform(6579) # transform CRS
```

-   We can now pull down ACS 5-year data from 2019 for Texas tracts

```{r}
pct_hispanic <- get_acs(
  geography = "tract",
  variables = "DP05_0071P",
  state = "TX",
  year = 2019,
  geometry = TRUE
) %>% 
  st_transform(6579)
```

-   We can now keep only the tracts we are interested in using a spatial join

#### Computing and visualizing the spatial join

-   with `st_join()`,dataset `x` retains geometry and gains attributes from dataset `y` based on spatial predicates

    -   `suffix` passes a suffix to be appended to columns with same names

    -   `left = FALSE` requests inner join

```{r}
hispanic_by_metro <- st_join(
  pct_hispanic,
  tx_cbsa,
  join = st_within,
  suffix = c("_tracts", "_metro"),
  left = FALSE
)
```

-   We can now perform group-wise data visualization and analysis across metro areas

```{r metro-facet}
hispanic_by_metro %>% 
  mutate(NAME_metro = str_replace(NAME_metro, ", TX Metro Area", "")) %>% 
  ggplot() +
  geom_density(aes(x = estimate_tracts), color = "navy", fill = "navy",
               alpha = 0.4) +
  theme_minimal() +
  facet_wrap(~NAME_metro) +
  labs(title = "Distribution of Hispanic/Latino population by Census tract",
       subtitle = "Largest metropolitan area in Texas",
       y ="Kernel density estimate",
       x = "Percent Hispanic/Latino in Census tract")
```

-   We can also generate group-wise summary statistics

```{r median-group-wise}
median_by_metro <- hispanic_by_metro %>% 
  group_by(NAME_metro) %>% 
  summarize(median_hispanic = median(estimate_tracts, na.rm = TRUE))

median_by_metro
```

-   Note: the above `summarize()` operation also dissolves group geographies

## 7.3 Small area time-series analysis

-   We may occasionally want to perform time-series analysis of geographies smaller than Counties.

    -   These areas are more prone to change in shape over time

        -   The Census redraws census tracts if they grow too large between decennial Censuses, usually by subdividing the existing Census tracts

    -   one method of addressing the change-over-time is *areal interpolation*

        -   data is allocated from one set of zones to a second set of overlapping zones using weighted reallocation

            -   Two common methods for interpolating in the overlapping areas:

                -   *area-weighted interpolation:* define weights by the area of geometry overlaps; implemented through `sf::st_interpolate_aw()`

                    -   assumes uniform distribution of attributes

                -   *population-weighted interpolation:* defines weights from a third dataset that represents population estimates, and estimates the population in geometry overlaps using these weights. implemented through `tidycensus::interpolate_pw()`

                    -   Requires three datasets - the two overlapping datasets, and a third dataset with population estimates to act as weights

                    -   The implemention in **tidycensus** uses a *weighted block point approach:*

                        -   census blocks and their population estimates are converted to points

                        -   these points are aggregated in origin/destination intersections to produce pop weights

```{r wfh-data-access}
### Pull 2011-2015 5 year estimates
wfh_15 <- get_acs(
  geography = "tract",
  variables = "B08006_017",
  year = 2015,
  state = "AZ",
  county = "Maricopa",
  geometry = TRUE
) %>%
  select(estimate) %>%
  st_transform(26949) # CRS: NAD 83 / Arizona Central

### Pull 2016-2020 5 year estimates
wfh_20 <- get_acs(
  geography = "tract",
  variables = "B08006_017",
  year = 2020,
  state = "AZ",
  county = "Maricopa",
  geometry = TRUE
 ) %>%
  st_transform(26949) # CRS: NAD 83 / Arizona Central
```

```{r area-weighted}
wfh_interpolate_aw <- st_interpolate_aw(
  wfh_15,
  wfh_20,
  extensive = TRUE # weighted sums instead of weighted means
) %>% 
  mutate(GEOID = wfh_20$GEOID) # use 2020 census tract IDs

wfh_interpolate_aw
```

```{r population-weighted}
## First, assign the blocks - tigris pulls down POP20 and HOUSING20 data
maricopa_blocks <- blocks(
  state = "AZ",
  county = "Maricopa",
  year = 2020
)

## Then we do population-weighted interpolation
wfh_interpolate_pw <- interpolate_pw(
  wfh_15,
  wfh_20,
  to_id = "GEOID",
  extensive = TRUE, # sum rather than mean
  weights = maricopa_blocks, # define the blocks for block->point conversion
  weight_column = "POP20", # define the variable to use as the weight
  crs = 26949
)
```

### Making small-area comparisons

-   we've interpolated 2011-2015 estimates to 2020 census tracts.

-   We may now compare the 2015 5 Year and the 2020 5 Year

    -   we do this using a left join

        -   remember to maintain the left geometry, and provide a suffix

```{r 2015-2020-comparison}
wfh_shift <- wfh_20 %>%
  left_join(st_drop_geometry(wfh_interpolate_pw), # drop the geometry
            by = "GEOID",
            suffix = c("_2020", "_2015")) %>% # add the suffixes
  mutate(wfh_shift = estimate_2020 - estimate_2015) # calculate the change

ggplot() +
  geom_sf(data = wfh_shift, aes(fill = wfh_shift), color = NA,
          alpha = 0.8) +
  scale_fill_distiller(palette = "PuOr", direction = -1) +
  labs(fill = "Shift, 2011-2015 to\n2016-2020 ACS",
       title = "Change in work-from-home population",
       subtitle = "Maricopa County, Arizona") +
  theme_void()
```

## 7.4 Distance and Proximity Analysis

-   We will calculate the distance between census tracts and Level I and Level II Trauma Hospitals in Iowa

    -   First, we need the census tracts and the hospital locations

```{r iowa-census-hospitals}
library(tigris)
library(sf)
library(tidyverse)
options(tigris_use_cache = TRUE)

# CRS: NAD83 / Iowa North
ia_tracts <- tracts("IA", cb = TRUE, year = 2019) %>%
  st_transform(26975)

hospital_url <- "https://services1.arcgis.com/Hp6G80Pky0om7QvQ/arcgis/rest/services/Hospital/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson"

trauma <- st_read(hospital_url) %>%
  filter(str_detect(TRAUMA, "LEVEL I\\b|LEVEL II\\b|RTH|RTC")) %>%
  st_transform(26975) %>%
  distinct(ID, .keep_all = TRUE)

names(trauma)
```

### Calculating distance

-   to calculate Euclidian distance of hospitals, we want to identify hospitals in and around Iowa - this includes hospitals in other states.

    -   first, we'll do a spatial filter with a distance threshold to capture those within Iowa or within a certain distance of Iowa's boundaries

        -   this is done with `st_is_within_distance` predicate set to 100000 meters, or 10Km

```{r hosp-near-iowa}
ia_trauma <- trauma %>%
  st_filter(ia_tracts, 
            .predicate = st_is_within_distance,
            dist = 100000)

ggplot() + 
  geom_sf(data = ia_tracts, color = "NA", fill = "grey50") + 
  geom_sf(data = ia_trauma, color = "red") + 
  theme_void()
```

-   now we use `st_distance()` to calculate distances from census tract centroids

    -   this will generate a (dense) distance matrix of distances

```{r distance-matrix}
dist <- ia_tracts %>%
  st_centroid() %>%
  st_distance(ia_trauma) 

dist[1:5, 1:5]
```

-   to find the shortest distance for each tract to a hospital, we can run the following:

```{r minimum-distances}
min_dist <- dist %>% # take the distance matrix
  apply(1, min) %>%  # apply the min() function to each row in the matrix
  as.vector() %>%    # return the minimum as a vector of distances
  magrittr::divide_by(1000) # divide each element of the vector by 1000 to return KM

hist(min_dist) # plot
```

### Calculating Travel Times

-   This portion requires the use of a routing engine, and I will need to see about performing the analysis using an alternative service besides MapBox

### Catchment areas with buffers and isochrones

-   We'll perform a buffer catchment around Iowa Methodist Medical Center in Des Moines

```{r}
iowa_methodist <- filter(ia_trauma, ID == "0009850308")

buf5km <- st_buffer(iowa_methodist, dist = 5000)
```

-   the second portion of this exercise requires the development of isochrones - as I am not using the Mapbox API I will need to find another way of performing that aspect. In the meantime, I will demonstrate the buffer catchment below.

```{r}
library(leaflet)
library(leafsync)

hospital_icon <- makeAwesomeIcon(
  icon = "ios-medical",
  markerColor = "red",
  library = "ion"
)

# Convert data to CRS 4326 for Leaflet
map1 <- leaflet() %>% 
  addTiles() %>% 
  addPolygons(data = st_transform(buf5km, 4326)) %>% 
  addAwesomeMarkers(data = st_transform(iowa_methodist, 4326),
                    icon = hospital_icon)

map1
```

### Computing demographic estimates for zones with areal interpolation

-   First, we get the demography data for the data - in this case, poverty and total population

    -   Remember to transform it to the CRS we'll be using

```{r}
polk_poverty <- get_acs(
  geography = "block group",
  variables = c(poverty_denom = "B17010_001",
                poverty_num = "B17010_002"),
  state = "IA",
  county = "Polk",
  geometry = TRUE,
  output = "wide",
  year = 2020
) %>% 
  select(poverty_denomE, poverty_numE) %>% 
  st_transform(26975)
```

-   Then we get the

```{r}
library(glue)

polk_blocks <- blocks(
  state = "IA",
  county = "Polk",
  year = 2020
)

buffer_pov <- interpolate_pw(
  from = polk_poverty,
  to = buf5km,
  extensive = TRUE,
  weights = polk_blocks,
  weight_column = "POP20",
  crs = 26975
) %>% 
  mutate(pct_poverty = 100 * (poverty_numE / poverty_denomE))
```

## 7.5 Better cartography with spatial overlay

-   As noted before, **tidycensus** pulls down Census Geographies using **tigris**, and by default, it pulls down *cartographic boundary files*Â with coastlines pre-clipped.

-   We may rather decide to pull down core shapefiles and erase water ourselves, say in cases where the Cartographic Boundary File includes water in the scale it is drawn at:

```{r}
library(tidycensus)
library(tidyverse)
options(tigris_use_cache = TRUE)

ny <- get_acs(
  geography = "tract", 
  variables = "B19013_001", 
  state = "NY", 
  county = "New York", 
  year = 2020,
  geometry = TRUE
)

ggplot(ny) + 
  geom_sf(aes(fill = estimate)) + 
  scale_fill_viridis_c(labels = scales::label_dollar()) + 
  theme_void() + 
  labs(fill = "Median household\nincome")
```

### "Erasing" areas from Census polygons

We can perform this operation as such:

1.  acquire the full polygons by setting \`cb = FALSE\` in the `get_acs()` function call

```{r}
ny2 <- get_acs(
  geography = "tract",
  variables = "B19013_001",
  state = "NY",
  county = "New York",
  geometry = TRUE,
  year = 2020,
  cb = FALSE
  ) %>% 
  st_transform(6538)
```

2.  Then, we perform an `erase_water()` operation to remove the water areas
    -   `tigris::erase_water()` auto-detects US Counties that contain the dataset
    -   it then pulls down a shapefile of the water boundaries
    -   it then erases the water areas from the dataset

```{r}
tictoc::tic()
ny_erase <- erase_water(ny2)
tictoc::toc()
```

## 7.6 Spatial neighborhoods and spatial weights matrices

-   The following chunk brings in the dataset we will be demonstrating with, which is the estimated median age in the Dallas-Fortworth area at the census tract level for the 2020 5-year vintage.

```{r tex-census}
library(tidycensus)
library(tidyverse)
library(tigris)
library(sf)
library(spdep)
options(tigris_use_cache = TRUE)

# CRS: NAD83 / Texas North Central
dfw <- core_based_statistical_areas(cb = TRUE, year = 2020) %>% 
  filter(str_detect(NAME, "Dallas")) %>% 
  st_transform(32138)

dfw_tracts <- get_acs(
  geography = "tract",
  variables = "B01002_001",
  state = "TX",
  year = 2020,
  geometry = TRUE
  ) %>% 
  st_transform(32138) %>% 
  st_filter(dfw, .predicate = st_within) %>% 
  na.omit()

ggplot(dfw_tracts) +
  geom_sf(aes(fill = estimate), color = NA) +
  scale_fill_viridis_c() +
  theme_void()
```

### Understanding spatial neighborhoods

-   in EDSA, the *neighborhood* is a representation of how a geographic feature is in relation with nearby features

-   Key r package for EDSA is **spdep**, which defines the following neighborhood definitions:

    -   *Proximity-based neighbors*: neighborhood is defined by distance threshold, or by a count of k number of closest features ( *k-nearest neighbor)*

    -   *Graph-based neighbors:* network-based definition

    -   *Contiguity-based neighbors:* shared edges and vertices of polygons define neighbors (i.e. queens-case neighbors and rooks-case neighbors)

        -   The learning example will use queen's case contiguity-based neighborhood

1.  We'll define the neighbors using `spdep::poly2nb()`

```{r dallas-neighbors}
neighbors <- poly2nb(dfw_tracts, queen = TRUE) # construct a neighborhood list
summary(neighbors)
```

-   Looking at the summary, we can see descriptive stats for neighborhood relations

    -   be wary of edge effects, as we have artificially removed neighbors by our definition of the area of study.

2.  Now we can visualize neighborhood relationships

```{r plot-relationships}
dfw_coords <- dfw_tracts %>% 
  st_centroid() %>% 
  st_coordinates()

plot(dfw_tracts$geometry)
plot(neighbors,
     coords = dfw_coords,
     add = TRUE,
     col = "blue",
     points = FALSE)
```

3.  We can identify the row indices of specific neighbors to a given census tract by calling the census tract's row.

```{r neighbor-check}
neighbors[[1]]
```

### Generating the spatial weights matrix

We'll develop spatial weights from the neighbors list.

-   we use `spdep::nb2listw()` to generate weights.

    -   by default, we use `style = "W"` - row-standardized weights, where each object's neighbors sum weights to 1.

    -   the alternate option, `style = "B"` gives binary weights - 0 for non-neighbors, and 1 for neighbors.

```{r weights}
weights <- nb2listw(neighbors, style = "W")

weights$weights[[1]]
```

## 7.7 Global and local spatial autocorrelation

-   given a row-standardized spatial weights object, we may now perform exploratory spatial data analysis of median age in the DFW metro.

    -    ***spatial autocorrelation*** is the relationship between a feature's attributes, and the attributes of its neighbors

        -   "we might be interested in the degree to which ACS estimates are *similar to* **or** *differ from* those of their neighbors ***as defined by a weights matrix***

-   The common patterns of spatial autocorrelation are:

    -   **Spatial clustering:** *data values trend toward similarity among neighbors*

    -   **Spatial uniformity**: *data values trend toward difference among neighbors*

    -   **Spatial randomness:** *data values between neighbors have no **apparent** relationships*

### Spatial lags and Moran's *I*

-   We use spatial weights matrices to estimate *spatial lag* - the effect of neighboring values on the values of an observation.

    -   row-standardized weights matrices can be used for lagged means; binary weights can produce lagged sums

-   We will use `spdep::lag.listw()`

    -   this function requires our vector and our weights list object

```{r}
dfw_tracts$lag_estimate <- lag.listw(weights, dfw_tracts$estimate)
```

-   We now can use the new `lag_estimate` column and our original variable, `estimate`, to create a scatterplot

```{r}
ggplot(dfw_tracts, aes(x = estimate, y = lag_estimate)) +
  geom_point(alpha = 0.3) +
  geom_abline(color = "red") +
  theme_minimal() +
  labs(title = "Median age by Census tract, Dallas-Fort Worth TX",
       x = "Median age",
       y = "Spatial lag, median age",
       caption = "Data source: 2016-2020 ACS via the tidycensus R package.\nSpatial relationships based on queens-case polygon contiguity.")
```

-   as the spatially lagged median age increases, so does the median age, as a general trend.

    -   We can check the significance of this relationship with a Moran's *I* test

```{r}
moran.test(dfw_tracts$estimate, weights)
```

-   A positive Moran's *I* implies a spatial clustering; the p-value being less than an 0.05 threshold means we reject the null hypothesis of spatial randomness.

-   As such, we may interpret this as a non-negligible trend wherein younger populations tend to be nearer other young populations, and older populations tend to be near older populations.

### Local spatial autocorrelation

-   Local spatial autocorrelation analysis can identify hot-spots and cold-spots by disaggregating global results.

    -   one common measure is Getis-Ord local G statistic

        -   the default G*i* statistic computes a ration of weighted average of neighborhood values to total sum of values

        -   G*i*\* includes the initial location, *i* , in the computation

```{r}
# to compute Gi* we need to use include.self()
localg_weights <- nb2listw(include.self(neighbors))

dfw_tracts$localG <- localG(dfw_tracts$estimate, localg_weights)

ggplot(dfw_tracts) + 
  geom_sf(aes(fill = as.double(localG)), color = NA) + 
  scale_fill_distiller(palette = "RdYlBu") + 
  theme_void() + 
  labs(fill = "Local Gi* statistic")
```
